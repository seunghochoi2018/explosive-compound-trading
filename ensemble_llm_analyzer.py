#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
 LLM  - 14b Ã— 4  

===   (RAM 28GB ) ===
14b Ã— 4 = 4  (96 )
- Layer 1-24 (14b Ã— 1):   (1, 7GB)
- Layer 25-48 (14b Ã— 1): ~  (5~30, 7GB)
- Layer 49-72 (14b Ã— 1):   (1, 7GB)
- Layer 73-96 (14b Ã— 1):   ( , 7GB)

===    ( ) ===

1.   (93% )
   -  : 150 â†’ 20  (L{range}:{decision}({conf}%))
   -  : 1 10â†’5, 5 20â†’8,  50â†’10
   -  :  (41,315 chars â†’ 2,000 chars)

2. RAM   (40GB  28GB )
   - 14b: num_ctx=32768, num_batch=8192 (7GB Ã— 4 = 28GB)
   -  : use_mmap=True, use_mlock=True ( )

3.   
   - 14b: num_predict=250 ( )
   -  : stop=["\n\n", "```", "##", "}"]

4.   
   - top_k: 40â†’5 (  87% )
   - top_p: 0.9â†’0.3 (  66% )
   - tfs_z: 1.0â†’0.5 (Tail Free Sampling)
   - typical_p: 0.7 (Typical Sampling)

5.   
   - repeat_penalty=1.0 (  )
   - presence_penalty=0.0 (  )
   - frequency_penalty=0.0 (  )
   - penalize_newline=False (  )
   - mirostat=0 (Mirostat  )

6. GPU/CPU 
   - num_gpu=99 ( GPU  )
   - main_gpu=0 ( GPU )
   - num_thread: 1.5b=8, 14b=12 ( )
   - num_parallel=4 (14b 2  )
   - numa=True (NUMA )

7.  
   - f16_kv=True (FP16 KV ,  2)
   - low_vram=False (VRAM  )
   - num_keep=4 ( )
   - logits_all=False (   )

8.   
   - keep_alive=60m (60 RAM )
   -       ( )

===   ===
- 14b Ã— 2 + 1.5b Ã— 1 = 3 
-  : 14b(118) Ã— 2 + 1.5b(10) =  250 (4)
-  : 180 (3)

 :        ( )
 : Layer 65-96 (14b)  
"""

import json
import requests
import time
from typing import Dict, List, Optional
from datetime import datetime
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

class EnsembleLLMAnalyzer:
    """13 LLM    ( +  )"""

    def __init__(self, base_model="qwen2.5:14b"):
        self.base_model = base_model
        self.power_model = base_model  # [FIX] base_model íŒŒë¼ë¯¸í„° ì‚¬ìš©
        self.ollama_url = "http://localhost:11435"  # KIS ì „ìš© Ollama ì„œë²„

        print("[MAX_PERFORMANCE] 14b Ã— 2   ")
        print("[MAX_PERFORMANCE] RAM 14GB  (7GB )")
        print("[MAX_PERFORMANCE]   : 4")
        # self._preload_models()  # [DISABLED] ë©ˆì¶¤ í˜„ìƒìœ¼ë¡œ ë¹„í™œì„±í™”
        print("[INFO] _preload_models() ê±´ë„ˆëœ€")

        # 2    (14b Ã— 2)
        # [FIX] base_model íŒŒë¼ë¯¸í„° ì‚¬ìš© (í•˜ë“œì½”ë”© ì œê±°)
        self.models = {
            "layer_1_48": {  # Layer 1-48:  +
                "role": f" / ({base_model})",
                "focus": "1~15 ,  +",
                "timeframe": "1m-15m",
                "temperature": 0.10,
                "layer_emulation": "1-48",
                "model": base_model
            },
            "layer_49_96": {  # Layer 49-96:  +
                "role": f" / ({base_model})",
                "focus": " ,  +",
                "timeframe": "all",
                "temperature": 0.30,
                "layer_emulation": "49-96",
                "model": base_model
            }
        }

        print(f"[ENSEMBLE] 1 LLM (14b Ã— 1)   ")
        print(f"  - Layer 49-96: {self.power_model} ( +)")
        print(f"  - : ~7GB (14b:7GB Ã— 1)")
        print(f"  - :  45 ")

    def analyze_sequential(self, market_data: Dict, trade_history: List[Dict],
                          meta_insights: List[Dict]) -> Dict:
        """
        3    (14b Ã— 3 = 96 14b )

        Layer 1-32 â†’ 33-64 â†’ 65-96
               ( )

        Returns:
            {
                "final_decision": "BUY/SELL/HOLD",
                "final_confidence": 85,
                "reasoning": "  ...",
                "layer_results": [...]
            }
        """
        start_time = time.time()

        # [14b Ã— 1] 1ê°œ ë ˆì´ì–´ ì‹¬ì¸µ ë¶„ì„ (ë¹ ë¥¸ ë¶„ì„)
        print(f"\n{'='*70}")
        print(f"[v7.4 SINGLE] 1 LLM (14b Ã— 1) ì•™ìƒë¸” ì‹œì‘...")
        print(f"{'='*70}")
        print(f"[DEBUG] ëª¨ë¸: 14b Ã— 1 (ë¹ ë¥¸ ë¶„ì„)")
        print(f"[DEBUG] ì‹¤í–‰: Layer 49-96 (ì „ëµ + ë¦¬ìŠ¤í¬)")
        print(f"[DEBUG] ì˜ˆìƒ ì‹œê°„: ì•½ 45ì´ˆ")
        print(f"[DEBUG] market_data keys: {list(market_data.keys())}")
        print(f"[DEBUG] trade_history ê°œìˆ˜: {len(trade_history)}")
        print(f"[DEBUG] meta_insights ê°œìˆ˜: {len(meta_insights)}")

        # 1ê°œ ë ˆì´ì–´ ì‹¤í–‰ (ì „ëµ + ë¦¬ìŠ¤í¬)
        sequence = [
            "layer_49_96"    # 49-96: ì „ëµ + ë¦¬ìŠ¤í¬
        ]

        results = []
        accumulated_context = ""  # ì´ì „ ë ˆì´ì–´ ê²°ê³¼ ì¶•ì 

        for i, model_key in enumerate(sequence, 1):
            model_info = self.models[model_key]
            layer_range = model_info["layer_emulation"]
            model_name = model_info.get("model", self.base_model)

            print(f"\n[ENSEMBLE {i}/1] Layer {layer_range} | ëª¨ë¸: {model_name} | ì—­í• : {model_info['role']}")
            print(f"[DEBUG] Ollama API í˜¸ì¶œ ì¤€ë¹„...")
            print(f"[DEBUG] _analyze_single_model í˜¸ì¶œ ì‹œì‘")

            # ë‹¨ì¼ ëª¨ë¸ ë¶„ì„
            result = self._analyze_single_model(
                model_key,
                model_info,
                market_data,
                trade_history,
                meta_insights,
                previous_context=accumulated_context
            )

            results.append(result)

            #      (:  )
            accumulated_context += f"L{layer_range}:{result['decision']}({result['confidence']}%) "

            print(f"[ENSEMBLE Layer {layer_range}] : {result['decision']} ({result['confidence']}%)")

        #  (strategist)    
        final_layer = results[-1]

        #    
        votes = {"BUY": 0, "SELL": 0, "HOLD": 0}
        for r in results:
            votes[r["decision"]] += 1

        #
        layer_summary = "\n".join([
            f"- {r.get('role', 'Unknown Layer')} (Layer {self.models[list(self.models.keys())[i]]['layer_emulation']}): "
            f"{r['decision']} ({r['confidence']}%)"
            for i, r in enumerate(results)
        ])

        final_reasoning = (
            f"===    (32b ) ===\n"
            f"{layer_summary}\n\n"
            f"  :\n"
            f"{final_layer['reasoning']}"
        )

        elapsed = time.time() - start_time

        print(f"[ENSEMBLE]  : {final_layer['decision']} "
              f"({final_layer['confidence']}%) | "
              f" : {votes} | {elapsed:.1f}")

        return {
            "final_decision": final_layer["decision"],
            "final_confidence": final_layer["confidence"],
            "votes": votes,
            "reasoning": final_reasoning,
            "layer_results": results,
            "elapsed_time": round(elapsed, 1)
        }

    def analyze_parallel(self, market_data: Dict, trade_history: List[Dict],
                        meta_insights: List[Dict]) -> Dict:
        """
           sequential  
        """
        return self.analyze_sequential(market_data, trade_history, meta_insights)

    def _analyze_single_model(self, model_key: str, model_info: Dict,
                             market_data: Dict, trade_history: List[Dict],
                             meta_insights: List[Dict], previous_context: str = "") -> Dict:
        """ë‹¨ì¼ ëª¨ë¸ ë¶„ì„ (í”„ë¡¬í”„íŠ¸ + LLM í˜¸ì¶œ)"""

        print(f"[DEBUG] _analyze_single_model ì§„ì… (model_key: {model_key})")

        # ì—­í• ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        print(f"[DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘...")
        prompt = self._create_role_specific_prompt(
            model_key, model_info, market_data, trade_history, meta_insights,
            previous_context
        )
        print(f"[DEBUG] í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)} chars)")

        #   temperature 
        temperature = model_info.get("temperature", 0.3)
        model_to_use = model_info.get("model", self.base_model)  #  

        # LLM í˜¸ì¶œ
        try:
            # ë””ë²„ê¹…: í”„ë¡¬í”„íŠ¸ í¬ê¸°
            prompt_size = len(prompt)
            print(f"[DEBUG] í”„ë¡¬í”„íŠ¸: {prompt_size} chars | ëª¨ë¸: {model_to_use} | Temperature: {temperature}")
            if prompt_size > 5000:
                print(f"[WARNING] í”„ë¡¬í”„íŠ¸ ë§¤ìš° í¼! ({prompt_size} chars)")

            from datetime import datetime
            print(f"[DEBUG] ===== íì‰ ë””ë²„ê¹… =====")
            print(f"[DEBUG] ìš”ì²­ URL: {self.ollama_url}/api/generate")
            print(f"[DEBUG] ëª¨ë¸: {model_to_use}")
            print(f"[DEBUG] ì‹œê°„: {datetime.now().strftime('%H:%M:%S')}")

            # í¬íŠ¸ í™•ì¸
            import socket
            port = self.ollama_url.split(':')[-1].split('/')[0]
            print(f"[DEBUG] í¬íŠ¸: {port}")

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex(('127.0.0.1', int(port)))
                sock.close()
                if result == 0:
                    print(f"[DEBUG] í¬íŠ¸ {port} ì—°ê²° OK")
                else:
                    print(f"[DEBUG] í¬íŠ¸ {port} ì—°ê²° ì‹¤íŒ¨!")
            except Exception as e:
                print(f"[DEBUG] í¬íŠ¸ í™•ì¸ ì˜¤ë¥˜: {e}")

            print(f"[DEBUG] Ollama API í˜¸ì¶œ ì‹œì‘ ({self.ollama_url})")

            api_start = time.time()
            timeout_sec = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ

            # ì§„í–‰ìƒí™© í‘œì‹œ ì“°ë ˆë“œ ì‹œì‘
            progress_stop = threading.Event()
            progress_start_time = time.time()

            def show_progress():
                while not progress_stop.is_set():
                    progress_stop.wait(10)  # 10ì´ˆë§ˆë‹¤
                    if not progress_stop.is_set():
                        elapsed = int(time.time() - progress_start_time)
                        minutes = elapsed // 60
                        seconds = elapsed % 60
                        if minutes > 0:
                            print(f"  ... ë¶„ì„ ì§„í–‰ ì¤‘: {minutes}ë¶„ {seconds}ì´ˆ ê²½ê³¼ (ì˜ˆìƒ: 1-3ë¶„)")
                        else:
                            print(f"  ... ë¶„ì„ ì§„í–‰ ì¤‘: {seconds}ì´ˆ ê²½ê³¼ (ì˜ˆìƒ: 1-3ë¶„)")

            progress_thread = threading.Thread(target=show_progress, daemon=True)
            progress_thread.start()

            # Ollama API í˜•ì‹
            print(f"[DEBUG] ===== HTTP ìš”ì²­ ì „ì†¡ =====")
            print(f"[DEBUG] POST {self.ollama_url}/api/generate")
            print(f"[DEBUG] Body: model={model_to_use}, temp={temperature}, stream=False")
            print(f"[DEBUG] Timeout: {timeout_sec}ì´ˆ")

            import subprocess
            # Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
            try:
                result = subprocess.run(['netstat', '-ano'], capture_output=True, text=True, timeout=3)
                for line in result.stdout.split('\n'):
                    if '11435' in line and 'LISTEN' in line:
                        print(f"[DEBUG] í¬íŠ¸ 11435 ìƒíƒœ: {line.strip()}")
            except:
                pass

            print(f"[DEBUG] ìš”ì²­ ì „ì†¡ ì¤‘... (ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%H:%M:%S.%f')[:-3]})")

            try:
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_to_use,
                        "prompt": prompt,
                        "temperature": temperature,
                        "stream": False,
                        "keep_alive": "5m"  # 5ë¶„ RAM í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    },
                    timeout=timeout_sec
                )
                print(f"[DEBUG] ì‘ë‹µ ìˆ˜ì‹ ! (ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%H:%M:%S.%f')[:-3]})")
            except requests.exceptions.Timeout as e:
                print(f"[ERROR] ===== íƒ€ì„ì•„ì›ƒ ë°œìƒ! =====")
                print(f"[ERROR] {timeout_sec}ì´ˆ íƒ€ì„ì•„ì›ƒ")
                print(f"[ERROR] ì˜ˆì™¸: {e}")
                raise
            except Exception as e:
                print(f"[ERROR] ===== HTTP ìš”ì²­ ì‹¤íŒ¨! =====")
                print(f"[ERROR] ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
                print(f"[ERROR] ì˜ˆì™¸ ë‚´ìš©: {e}")
                raise

            api_elapsed = time.time() - api_start

            # ì§„í–‰ìƒí™© í‘œì‹œ ì“°ë ˆë“œ ì¤‘ì§€
            progress_stop.set()
            progress_thread.join(timeout=1)

            print(f"[DEBUG] ===== HTTP ì‘ë‹µ ì²˜ë¦¬ =====")
            print(f"[DEBUG] Status: {response.status_code}")
            print(f"[DEBUG] Headers: {dict(response.headers)}")

            if response.status_code == 200:
                result = response.json()
                llm_output = result.get('response', '')

                minutes = int(api_elapsed) // 60
                seconds = int(api_elapsed) % 60
                time_str = f"{minutes}ë¶„ {seconds}ì´ˆ" if minutes > 0 else f"{seconds}ì´ˆ"

                # íì‰ ë””ë²„ê¹…: ì‘ë‹µ ì •ë³´
                print(f"[DEBUG] ===== ì‘ë‹µ ì™„ë£Œ =====")
                print(f"[DEBUG] í¬íŠ¸ {port} ì‘ë‹µ ì™„ë£Œ")
                print(f"[DEBUG] ì†Œìš” ì‹œê°„: {time_str}")
                if api_elapsed > 120:  # 2ë¶„ ì´ìƒ
                    print(f"[WARNING] íì‰ ì˜ì‹¬! ì •ìƒ: 30-90ì´ˆ, ì‹¤ì œ: {time_str}")

                print(f"[ì™„ë£Œ] ë¶„ì„ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {time_str}, ì‘ë‹µ ê¸¸ì´: {len(llm_output)} chars)")

                # JSON
                parsed = self._parse_llm_response(llm_output)
                parsed["model"] = model_key
                parsed["role"] = model_info["role"]

                return parsed
            else:
                raise Exception(f"HTTP {response.status_code}")

        except requests.exceptions.Timeout:
            progress_stop.set()  # ì§„í–‰ìƒí™© í‘œì‹œ ì¤‘ì§€
            elapsed = time.time() - api_start
            print(f"[ERROR] Ollama ! (: {elapsed:.0f}, : {prompt_size} chars)")
            return {
                "model": model_key,
                "decision": "HOLD",
                "confidence": 50,
                "reasoning": f"Ollama  ({elapsed:.0f})"
            }
        except Exception as e:
            progress_stop.set()  # ì§„í–‰ìƒí™© í‘œì‹œ ì¤‘ì§€
            elapsed = time.time() - api_start
            print(f"[ERROR] LLM  : {e} (: {elapsed:.0f})")
            return {
                "model": model_key,
                "decision": "HOLD",
                "confidence": 50,
                "reasoning": f"LLM  : {e}"
            }

    def _create_role_specific_prompt(self, model_key: str, model_info: Dict,
                                    market_data: Dict, trade_history: List[Dict],
                                    meta_insights: List[Dict], previous_context: str = "") -> str:
        """      (   )"""

        role = model_info["role"]
        focus = model_info["focus"]
        timeframe = model_info["timeframe"]
        layer_range = model_info.get("layer_emulation", "unknown")

        #
        current_price = market_data.get('current_price', 0)
        price_history_1m = market_data.get('price_history_1m', [])
        price_history_5m = market_data.get('price_history_5m', [])

        # [NEW] ê°•í™”ëœ ì¶”ì„¸ ë¶„ì„ ë©”íŠ¸ë¦­
        recent_peak = market_data.get('recent_peak', current_price)
        decline_from_peak = market_data.get('decline_from_peak_pct', 0)
        short_momentum = market_data.get('short_momentum_pct', 0)
        mid_momentum = market_data.get('mid_momentum_pct', 0)
        momentum_weakening = market_data.get('momentum_weakening', 0)
        pattern_signal = market_data.get('pattern_signal', 'íš¡ë³´')
        current_direction = market_data.get('current_direction', None)

        #     (: )
        context_section = ""
        if previous_context:
            context_section = f" : {previous_context}\n"

        # 8    ( ) -
        timeframe = model_info.get("timeframe", "all")

        #     +   [ê°•í™”]
        if timeframe == "1m":
            recent_prices = price_history_1m[-5:] if len(price_history_1m) >= 5 else price_history_1m
            change = ((current_price - recent_prices[0]) / recent_prices[0] * 100) if recent_prices else 0
            trend = "" if change > 0.5 else "" if change < -0.5 else ""

            # [NEW] ì¶”ì„¸ ë°˜ì „ ê²½ê³  ì¶”ê°€
            reversal_alert = ""
            if decline_from_peak < -1.5:
                reversal_alert = f" âš ï¸ê³ ì ëŒ€ë¹„{abs(decline_from_peak):.1f}%í•˜ë½"
            if momentum_weakening > 1.0:
                reversal_alert += f" âš ï¸ëª¨ë©˜í…€ì•½í™”{momentum_weakening:.1f}%"

            data_desc = f"1 {trend}({change:+.1f}%) {recent_prices}{reversal_alert}"

        elif timeframe == "5m":
            recent_prices = price_history_5m[-5:] if len(price_history_5m) >= 5 else price_history_5m
            change = ((current_price - recent_prices[0]) / recent_prices[0] * 100) if recent_prices else 0
            trend = "" if change > 1.0 else "" if change < -1.0 else ""

            # [NEW] ì¶”ì„¸ ë°˜ì „ ê²½ê³ 
            reversal_alert = ""
            if decline_from_peak < -2.0:
                reversal_alert = f" âš ï¸ê³ ì ${recent_peak:.1f}â†’í˜„ì¬í•˜ë½{abs(decline_from_peak):.1f}%"
            if pattern_signal == "í•˜ë½_íŒ¨í„´":
                reversal_alert += " âš ï¸í•˜ë½íŒ¨í„´ê°ì§€"

            data_desc = f"5 {trend}({change:+.1f}%) {recent_prices}{reversal_alert}"

        elif timeframe in ["15m", "30m", "1h"]:
            recent_prices = price_history_5m[-8:] if len(price_history_5m) >= 8 else price_history_5m
            change = ((current_price - recent_prices[0]) / recent_prices[0] * 100) if recent_prices else 0
            trend = "" if change > 1.5 else "" if change < -1.5 else ""

            # [NEW] ì¤‘ê¸° ì¶”ì„¸ ë°˜ì „ ë¶„ì„
            reversal_alert = ""
            if current_direction == "BULL" and decline_from_peak < -2.0:
                reversal_alert = f" ğŸ”´BULLí¬ì§€ì…˜ì¸ë° ê³ ì ëŒ€ë¹„ {abs(decline_from_peak):.1f}%í•˜ë½!"
            if current_direction == "BEAR" and decline_from_peak > -0.5:
                reversal_alert = f" ğŸŸ¢BEARí¬ì§€ì…˜ì¸ë° í•˜ë½ì—†ìŒ - ë°˜ì „?"

            data_desc = f" {trend}({change:+.1f}%) {recent_prices}{reversal_alert}"

        else:
            # //  -    +   [ê°•í™”]
            recent_prices = price_history_1m[-3:] if len(price_history_1m) >= 3 else price_history_1m
            loss_count = len([t for t in trade_history[-10:] if isinstance(t, dict) and t.get('pnl_percent', 0) < 0])
            win_rate = ((10-loss_count)/10*100) if trade_history else 50

            # [NEW] ì „ëµ ë ˆì´ì–´ì— ì¶”ì„¸ ë°˜ì „ ì‹ í˜¸ ê°•ì¡°
            reversal_info = ""
            if decline_from_peak < -2.0 and current_direction == "BULL":
                reversal_info = f" ğŸš¨BULLâ†’BEARì „í™˜ê³ ë ¤! ê³ ì ${recent_peak:.1f}ì—ì„œ{abs(decline_from_peak):.1f}%í•˜ë½"
            elif momentum_weakening > 1.5:
                reversal_info = f" âš ï¸ì¶”ì„¸ì•½í™”{momentum_weakening:.1f}% ì „í™˜ì‹ í˜¸"
            elif pattern_signal == "í•˜ë½_íŒ¨í„´" and current_direction == "BULL":
                reversal_info = f" âš ï¸í•˜ë½íŒ¨í„´ê°ì§€(í˜„ì¬BULL)"
            elif pattern_signal == "ìƒìŠ¹_íŒ¨í„´" and current_direction == "BEAR":
                reversal_info = f" âš ï¸ìƒìŠ¹íŒ¨í„´ê°ì§€(í˜„ì¬BEAR)"

            data_desc = f"{recent_prices} {win_rate:.0f}%{reversal_info}"

        #    
        recent_10 = trade_history[-10:] if len(trade_history) >= 10 else trade_history
        if recent_10:
            wins = len([t for t in recent_10 if isinstance(t, dict) and t.get('pnl_percent', 0) > 0])
            recent_win_rate = (wins / len(recent_10) * 100) if recent_10 else 0
            last_3_decisions = [t.get('decision', 'HOLD') if isinstance(t, dict) else 'HOLD' for t in trade_history[-3:]]
            stats_info = f"{recent_win_rate:.0f}% 3:{'/'.join(last_3_decisions)}"
        else:
            stats_info = ""

        #    ()
        strategy_hint = ""
        if meta_insights and len(meta_insights) > 0:
            #  3
            top_strategies = meta_insights[-3:]
            strategy_summary = []
            for s in top_strategies:
                if not isinstance(s, dict):
                    continue
                insights = s.get('insights', {})
                win_patterns = insights.get('winning_patterns', [])
                if win_patterns:
                    #
                    pattern = win_patterns[0] if isinstance(win_patterns, list) else str(win_patterns)[:50]
                    strategy_summary.append(pattern[:30])
            if strategy_summary:
                strategy_hint = f"\n:{';'.join(strategy_summary[:2])}"

        # [NEW] í¬ì§€ì…˜ ì •ë³´ ë° ì²­ì‚° íŒë‹¨ ì¶”ê°€
        position = market_data.get('position', 'NONE')
        position_pnl = market_data.get('position_pnl', 0)

        position_info = ""
        if position != 'NONE':
            position_info = f"\nğŸ”µí˜„ì¬í¬ì§€ì…˜: {position} | PNL: {position_pnl:+.2f}%"

        # [NEW] ì†ì‹¤ ê±°ë˜ íŒ¨í„´ ê²½ê³  (ìµœê·¼ 10ê°œ ê±°ë˜ì—ì„œ ì†ì‹¤ íŒ¨í„´ ì¶”ì¶œ)
        loss_patterns = []
        if trade_history:
            recent_losses = [t for t in trade_history[-20:] if isinstance(t, dict) and t.get('pnl_pct', 0) < -0.5]

            for loss in recent_losses[-5:]:  # ìµœê·¼ 5ê°œ ì†ì‹¤ë§Œ
                loss_trend_1m = loss.get('market_1m_trend', '')
                loss_trend_5m = loss.get('market_5m_trend', '')
                loss_side = loss.get('side', '')
                loss_pnl = loss.get('pnl_pct', 0)

                if loss_trend_1m or loss_trend_5m:
                    loss_patterns.append(
                        f"âš ï¸{loss_side}ì§„ì…({loss_trend_1m}/{loss_trend_5m})â†’ì†ì‹¤{loss_pnl:.1f}%"
                    )

        loss_warning = ""
        if loss_patterns:
            loss_warning = f"\nâŒê³¼ê±°ì‹¤íŒ¨:{';'.join(loss_patterns[:3])}"

        # [CORE_PHILOSOPHY] ì ˆëŒ€ì  ì„ê³„ì¹˜ ì„¤ì • ê¸ˆì§€ - LLM ììœ¨ í•™ìŠµ ê¸°ë°˜ íŒë‹¨
        # ì£¼ì„: ì†ì ˆì„ ì´ ì•„ë‹Œ ì´ìƒ ì ˆëŒ€ì  ì„ê³„ì¹˜ ì„¤ì • ê¸ˆì§€ ë° ìƒí™©ì— ëŒ€í•œ í•™ìŠµ í•„ìš”
        # ì£¼ì„: ì´ë ‡ê²Œ llmì—ê²Œ í•™ìŠµì„ ì‹œì¼œì•¼ì§€ ì ˆëŒ€ì  ì„ê³„ì¹˜ë¥¼ ì„¤ì •í•˜ë©´ ëì´ ì—†ë‹¤
        # ì´ìœ :
        # - ì‹œì¥ ìƒí™©ì€ ë™ì  (bull/bear/sideways)
        # - ê³ ì • ì„ê³„ì¹˜ëŠ” íŠ¹ì • ìƒí™©ì—ì„œë§Œ ìœ íš¨
        # - LLMì´ ê³¼ê±° ì„±ê³µ/ì‹¤íŒ¨ íŒ¨í„´ í•™ìŠµ â†’ ìƒí™©ë³„ ììœ¨ íŒë‹¨
        # - ì˜ˆ: íš¡ë³´ì¥ -1% vs ê¸‰ë½ì¥ -1% â†’ ì˜ë¯¸ ì™„ì „íˆ ë‹¤ë¦„
        #
        #   (:  +  + positions + failures + LLM autonomous learning)
        prompt = f"""{role} Layer{layer_range} | {focus}
{context_section}:${current_price} | {data_desc}
{stats_info}{position_info}{loss_warning}{strategy_hint}

âš¡ê·œì¹™ (LLM ììœ¨ íŒë‹¨ - ì ˆëŒ€ì  ì„ê³„ì¹˜ ì‚¬ìš© ê¸ˆì§€):
1. [í•µì‹¬] ì¶”ì„¸ì„  ëŒíŒŒ(trendline breakout) ëª…í™•íˆ ì¸ì‹!
   ğŸ’¡ ì‚¬ìš©ì í”¼ë“œë°±: "íš¡ë³´ì¥ íŒë‹¨ë³´ë‹¤ ì¶”ì„¸ëŒíŒŒë¥¼ ëª…í™•íˆ ì¸ì‹í•˜ëŠ”ê²Œ ë” ì¤‘ìš”"
   - ì´ìœ : íš¡ë³´/ì¶”ì„¸ êµ¬ë¶„ë³´ë‹¤ ì¶”ì„¸ì„  ëŒíŒŒ ê°ì§€ê°€ ì‹¤ì œ ìˆ˜ìµ í•µì‹¬
   - ì˜ˆ: ê³ ì  $4558 â†’ í˜„ì¬ $4421 (-3%) = ìƒìŠ¹ì¶”ì„¸ì„  í•˜í–¥ëŒíŒŒ â†’ SELL
   - íš¡ë³´ì¥ ì§„ì… ê¸ˆì§€ëŠ” ë¶€ì°¨ì , ì¶”ì„¸ëŒíŒŒ ë†“ì¹˜ë©´ ìˆ˜ìµ ê¸°íšŒ ìƒì‹¤
2. í¬ì§€ì…˜ ë³´ìœ  ì¤‘ ì²­ì‚° íŒë‹¨ (LLM ììœ¨ ê²°ì •):
   ğŸ’¡ ì ˆëŒ€ %ë¡œ íŒë‹¨ ê¸ˆì§€! PNL + ì‹œì¥ ë§¥ë½ + ê³¼ê±° í•™ìŠµìœ¼ë¡œ ì¢…í•© íŒë‹¨
   - ì‘ì€ ìˆ˜ìµì—ì„œ ì²­ì‚°? â†’ ì¶”ì„¸ ê°•í•˜ë©´ ìœ ì§€, ë°˜ë“± ì¡°ì§ ìˆìœ¼ë©´ ì²­ì‚°
   - í° ìˆ˜ìµì—ì„œ ì²­ì‚°? â†’ ì¶”ì„¸ ì§€ì†ì´ë©´ ìœ ì§€, ë˜ëŒë¦¼ ìœ„í—˜ ìˆìœ¼ë©´ ì²­ì‚°
   - ì†ì‹¤ ì¤‘ ì²­ì‚°? â†’ ì¶”ì„¸ ì „í™˜ ì‹ í˜¸ + ê³¼ê±° ì‹¤íŒ¨ íŒ¨í„´ ìœ ì‚¬ì„± ê²€í† 
   - ì˜ˆ: PNL +5%ì§€ë§Œ ê°•í•œ í•˜ë½ ì¶”ì„¸ ì§€ì† â†’ ìœ ì§€
   - ì˜ˆ: PNL +20%ì§€ë§Œ ë°˜ë“± ì‹œì‘ + ê³¼ê±° ì´ ì§€ì ì—ì„œ ìˆ˜ìµ ë°˜ë‚© â†’ ì²­ì‚°
3. ê³¼ê±° ì‹¤íŒ¨ íŒ¨í„´ íšŒí”¼: ë™ì¼í•œ ì‹œì¥ ìƒí™© ë°˜ë³µ ê¸ˆì§€
4. ì†ì‹¤/ì´ìµ íŒë‹¨ì€ LLMì´ ì‹œì¥ ë§¥ë½ ë³´ê³  ììœ¨ ê²°ì • (ê³ ì • % ê¸ˆì§€)

âš ï¸ ì„ íƒì§€ (4ê°€ì§€ë§Œ ì‚¬ìš© - ëª…í™•ì„± í–¥ìƒ):
1. HOLD: í˜„ì¬ í¬ì§€ì…˜ ìœ ì§€ (ë˜ëŠ” ê´€ë§)
2. CLOSE: í˜„ì¬ í¬ì§€ì…˜ ì²­ì‚°ë§Œ (ìˆ˜ìµ/ì†ì‹¤ ì‹¤í˜„, ì‹ ê·œ ì§„ì… ì—†ìŒ)
3. BUY: ë§¤ìˆ˜ ì§„ì… (í¬ì§€ì…˜ ìˆìœ¼ë©´ ì²­ì‚° í›„ ì§„ì…)
4. SELL: ë§¤ë„ ì§„ì… (í¬ì§€ì…˜ ìˆìœ¼ë©´ ì²­ì‚° í›„ ì§„ì…)

ğŸ’¡ ì˜ˆì‹œ:
- í˜„ì¬ SELL +30% â†’ CLOSE ì„ íƒ â†’ ìˆ˜ìµ ì‹¤í˜„
- í˜„ì¬ SELL +30% â†’ BUY ì„ íƒ â†’ ì²­ì‚° í›„ ë°˜ëŒ€ í¬ì§€ì…˜
- í˜„ì¬ SELL +30% â†’ HOLD ì„ íƒ â†’ í¬ì§€ì…˜ ìœ ì§€
- í¬ì§€ì…˜ ì—†ìŒ â†’ BUY ì„ íƒ â†’ ë§¤ìˆ˜ ì§„ì…

JSON í˜•ì‹:
{{"decision":"HOLD","confidence":60,"reasoning":"í•˜ë½ ì¶”ì„¸ ì§€ì†, í¬ì§€ì…˜ ìœ ì§€"}}
{{"decision":"CLOSE","confidence":80,"reasoning":"ë°˜ë“± ì¡°ì§, ìˆ˜ìµ ì‹¤í˜„"}}
{{"decision":"BUY","confidence":75,"reasoning":"ìƒìŠ¹ ì¶”ì„¸ ì „í™˜, ë§¤ìˆ˜ ì§„ì…"}}
{{"decision":"SELL","confidence":85,"reasoning":"í•˜ë½ ì¶”ì„¸ ì‹œì‘, ë§¤ë„ ì§„ì…"}}
"""

        return prompt

    def _parse_llm_response(self, llm_output: str) -> Dict:
        """LLM  """
        try:
            # JSON 
            start = llm_output.find('{')
            end = llm_output.rfind('}') + 1

            if start >= 0 and end > start:
                json_str = llm_output[start:end]
                parsed = json.loads(json_str)

                return {
                    "decision": parsed.get("decision", "HOLD"),
                    "confidence": int(parsed.get("confidence", 50)),
                    "reasoning": parsed.get("reasoning", "")
                }
        except:
            pass

        #     
        decision = "HOLD"
        confidence = 50

        if "BUY" in llm_output.upper() or "" in llm_output:
            decision = "BUY"
            confidence = 70
        elif "SELL" in llm_output.upper() or "" in llm_output:
            decision = "SELL"
            confidence = 70

        return {
            "decision": decision,
            "confidence": confidence,
            "reasoning": llm_output[:200]
        }

    def _vote_and_aggregate(self, results: List[Dict]) -> Dict:
        """ (Layer 13)   ( Transformer  )"""

        # Layer 13 ( )  
        #  GPT-4, Claude        
        final_layer_result = results[-1]  #   = Layer 13 (89-96)

        final_decision = final_layer_result.get("decision", "HOLD")
        final_confidence = final_layer_result.get("confidence", 50)

        #  100% (  )
        consensus = 100

        #  
        reasoning_parts = []
        for result in results:
            model = result.get("model", "unknown")
            role = result.get("role", "unknown")
            decision = result.get("decision", "HOLD")
            conf = result.get("confidence", 50)
            reasoning = result.get("reasoning", "")[:100]

            reasoning_parts.append(
                f"- {role} ({model}): {decision} ({conf}%) - {reasoning}"
            )

        final_reasoning = (
            f"4   : {votes}\n"
            f": {consensus:.0f}%\n"
            f" : {final_decision} ( {final_confidence}%)\n\n"
            f" :\n" + "\n".join(reasoning_parts)
        )

        return {
            "final_decision": final_decision,
            "final_confidence": final_confidence,
            "votes": votes,
            "consensus": round(consensus, 1),
            "reasoning": final_reasoning,
            "individual_results": results
        }

    def _preload_models(self):
        """       """
        # Ollama ì„œë²„ ì—°ê²° í™•ì¸
        print(f"[DEBUG] LM Studio URL: {self.ollama_url}")
        try:
            health_check = requests.get("http://localhost:11435/v1/models", timeout=5)
            print(f"[DEBUG] Ollama ì„œë²„ ì‘ë‹µ: {health_check.status_code}")
            if health_check.status_code == 200:
                print(f"[DEBUG] Ollama ë²„ì „: {health_check.json()}")
        except Exception as e:
            print(f"[WARNING] Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            print(f"[WARNING] Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”!")
            print(f"[WARNING] ì‹¤í–‰ ë°©ë²•: C:\\Users\\user\\AppData\\Local\\Programs\\Ollama\\ollama.exe serve")
            return

        # RAM_BOOST ë¹„í™œì„±í™” (ì²« ì‹¤í–‰ ì‹œ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼)
        print(f"[INFO] RAM_BOOST ê±´ë„ˆëœ€ (ì²« LLM í˜¸ì¶œ ì‹œ ìë™ ë¡œë”©ë¨)")

        # models_to_preload = [self.power_model]
        # for model in set(models_to_preload):
        #     try:
        #         print(f"[RAM_BOOST] {model}   ...")
        #         response = requests.post(
        #             self.ollama_url,
        #             json={"model": model, "prompt": "", "stream": False, ...},
        #             timeout=30
        #         )
        #         if response.status_code == 200:
        #             print(f"[RAM_BOOST] {model}  ! (RAM )")
        #     except Exception as e:
        #         print(f"[RAM_BOOST] {model}   : {e} ( )")
